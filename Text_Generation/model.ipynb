{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text length: 1614019 characters.\n"
     ]
    }
   ],
   "source": [
    "import fitz  \n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    \n",
    "    full_text = \"\"\n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc.load_page(page_num)  \n",
    "        full_text += page.get_text(\"text\")  \n",
    "    \n",
    "    return full_text\n",
    "\n",
    "pdf_path = r\"D:\\hugging_face\\perfix-tuning\\A Game Of Thrones - George R. R. Martin (1).pdf\"\n",
    "text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "with open(\"game_of_thrones_text.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(text)\n",
    "\n",
    "print(f\"Extracted text length: {len(text)} characters.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned text length: 1558962 characters.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\n\\d+\\n', '\\n', text)\n",
    "    \n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s.,!?\\'\"-]', '', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "cleaned_text = clean_text(text)\n",
    "\n",
    "with open(\"cleaned_game_of_thrones_text.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(cleaned_text)\n",
    "\n",
    "print(f\"Cleaned text length: {len(cleaned_text)} characters.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81ecdeb7d3d8484492e42265e7982d18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 02:00, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from datasets import Dataset\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "model_name = r\"D:\\hugging_face\\llama_3.2-1b_Model\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    load_in_8bit=False,  \n",
    "    torch_dtype=torch.float16,  \n",
    "    device_map=\"auto\"\n",
    ").to(device)\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,  \n",
    "    lora_alpha=32, \n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  \n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "def load_got_dataset(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        texts = f.read().split('\\n\\n')\n",
    "    return [{\"text\": text} for text in texts if text.strip()]\n",
    "\n",
    "dataset = load_got_dataset(r'D:\\hugging_face\\perfix-tuning\\cleaned_game_of_thrones_text.txt')\n",
    "dataset = Dataset.from_list(dataset)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples['text'], \n",
    "        truncation=True, \n",
    "        max_length=512, \n",
    "        padding='max_length'\n",
    "    )\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, \n",
    "    mlm=False  \n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./got_llama_lora_model',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    learning_rate=1e-4,\n",
    "    logging_dir='./logs',\n",
    "    save_strategy='steps',\n",
    "    save_steps=500,\n",
    "    fp16=True,  \n",
    "    dataloader_num_workers=4\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "model.save_pretrained('./got_llama_lora_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: In the cold lands beyond the Wall, a lone ranger\n",
      "Generated Text:\n",
      "In the cold lands beyond the Wall, a lone ranger has been roaming the woods for months. He has come across a mysterious and terrible place where the world has changed and where everything has become more dangerous. The ranger knows that he must get back to his people, but he is not sure how to get there. Can he find his way home?\n",
      "This is a story about a boy who has to overcome his fears and find a way to reach home.\n",
      "This book is for readers who like to read about adventure and mystery.\n",
      "It has a great plot, interesting characters, and an ending that will leave you wanting more!\n",
      "The story is written in a simple and easy to understand language, so even young readers can enjoy it.\n",
      "The illustrations are beautiful and will make you want to sit down and read the whole book!\n",
      "There are also some nice touches like the fact that the boy has his own notebook which he uses to write down his thoughts and feelings as he travels through the forest.\n",
      "Overall, this\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: The winter winds howled across the battlements of Winterfell\n",
      "Generated Text:\n",
      "The winter winds howled across the battlements of Winterfell, but Bran Stark still had one more winter to endure before he could put the Night King to rest.\n",
      "Winter is coming, and this time it’s not just the White Walkers who are coming to claim the Seven Kingdoms. The Night’s Watch has been infiltrated by the dead, the living, those undead, or the undead like. It’s a war without end. With a new threat in the form of the Mad King, a threat that will only grow as the winter progresses, Bran must fight to save his home and the people he loves.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: As the dragons circled overhead, the last Targaryen whispered\n",
      "Generated Text:\n",
      "As the dragons circled overhead, the last Targaryen whispered to his queen, “Do you remember how you felt when you first met your husband? Do you recall the first time you laid eyes on him?”\n",
      "The queen looked at her husband, and he smiled, a smile that was not a real smile. “I remember it like it was yesterday,” she said. Then she turned to the prince, who was leaning against the wall, watching the scene unfold with interest. She spoke to him softly, but the words were clear enough for everyone to hear.  “The first thing I noticed about my husband was his eyes. They were the color of the dragon’s eyes, fire and flame. He was beautiful, like a god.”\n",
      "The prince smiled at his mother. His eyes were as blue as the sky, just like his father’s. And the fire in his own eyes was just as bright as his dragon father had been. But he didn’t look at the queen\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Prompt: In the shadowy halls of King's Landing, a conspiracy was brewing\n",
      "Generated Text:\n",
      "In the shadowy halls of King's Landing, a conspiracy was brewing. A young boy named Arya Stark was training to become a warrior for House Stark, and he had been sent to the Night's Watch to learn the ways of the ice. Arysa, as he was known, was not yet eighteen, but his training would begin soon, when he would be given his first sword and his name would forever be written in the record books. But Arys' journey to training was a long one, even for a young man. In the meantime, his mother had died, leaving him to survive on his own. His father, who was the Lord Commander of all the men at the Watch, had sent Aryas away, telling him that he should find his way to a place of peace. He had no idea what that place was, or where it would take him. So Aryad, alone in a cold and empty world, began his journey.\n",
      "Arya is an orphan\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "model_name = r\"D:\\hugging_face\\llama_3.2-1b_Model\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "peft_model_id = \"./got_llama_lora_model\"\n",
    "model = PeftModel.from_pretrained(base_model, peft_model_id)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "def generate_text(prompt, max_length=200):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs, \n",
    "            max_length=max_length, \n",
    "            num_return_sequences=1,\n",
    "            temperature=0.7,  \n",
    "            top_p=0.9,        \n",
    "            no_repeat_ngram_size=2\n",
    "        )\n",
    "    \n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "prompts = [\n",
    "    \"In the cold lands beyond the Wall, a lone ranger\",\n",
    "    \"The winter winds howled across the battlements of Winterfell\",\n",
    "    \"As the dragons circled overhead, the last Targaryen whispered\",\n",
    "    \"In the shadowy halls of King's Landing, a conspiracy was brewing\"\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    try:\n",
    "        result = generate_text(prompt)\n",
    "        print(f\"\\nPrompt: {prompt}\\nGenerated Text:\\n{result}\")\n",
    "        print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred with prompt '{prompt}': {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
